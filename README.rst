The VRFT Technique
==================

**V**\ irtual **R**\ eference **F**\ eedback **T**\ uning is a data driven method to design a controller for an unknown plant based on input/output measurements. The method can be applied using a single set of input/output data generated by the plant without the need for specific experiments or model identification. 

How Does It Work ?
------------------

To use VRFT we need:

* a reference model, a model that describes the ideal behaviour of the closed loop system.
* one set of input-output data on the plant. The input does not need to have any special characteristics. 

.. figure:: {filename}/static/pages/about-vrft/vrft_block_diagram.png
    :align: center  
    :alt: The block diagram of VRFT

    Block diagram showing the closed loop control system and the reference model.

    Note that the output of the reference model and the control loop are identical.

Since the output :math:`y(t)` is known we can calculate an input signal :math:`r(t)` such that: 

.. math::
    
    y(t) = M_R(z) r(t)

This signal is the *virtual reference*. Using this virtual reference we can calculate the input to the controller: 

.. math::

    e(t) = r(t) - y(t)

Now that both the input and the output (:math:`u(t)`) of the controller are known the control problem can be reduced to an identification problem.


A More Rigorous approach
------------------------

It is assumed that the plant is a *linear* SISO discrete-time dynamical system described by a rational transfer function (:math:`P(z)`).

We choose a reference model :math:`M_R(z)` that describes the desired behaviour of the closed loop control system. 

We determine a class of controllers :math:`C(z; \theta)` that depend linearly on the parameter vector :math:`\theta` such that: 

.. math::

    C(z; \theta) = \beta(z)^T\theta^T

Where: 

* :math:`\beta = [\beta_1(z), \beta_2(z), ..., \beta_n(z)]^T` is a vector of linear transfer functions
* :math:`\theta = [\theta_1, \theta_2, ... \theta_n]^T` is the vector of parameters
  
The control objective is the minimisation of the following criterion: 

.. math:: 

    \begin{aligned}
        J_{MR}(\theta) &= \left| \left( \frac{P(z)C(z; \theta)}{1 + P(z)C(z; \theta)} - M(z) \right) W(z) \right|_2^2 \\
                       &= \left| \left( T(z) - M(z) \right) W(z) \right|_2^2
    \end{aligned}

which is equivalent to minimising the difference between the closed loop system scaled by an appropriate weighting function. The choice of :math:`W(z)` is up to the system designer and can be chosen to emphasize or de-emphasize performance in certain frequency bands. 

An optimal choice of :math:`C(z; \theta)` will produce a closed loop system whose transfer function is :math:`M(z)`. Thus, if the closed-loop system is fed a reference signal :math:`r(t)` its output will be :

.. math:: 

    y(t) = T(z)r(t) = M(z)r(t)

The VRFT method works by using the measured output (:math:`y(t)`) to generate a reference signal :math:`\bar{r}(t)` such that: 

.. math::

    M(z)\bar{r}(t) = y(t)

This is our so-called *virtual* reference, it does not exist in the physical world and is only used a tool to generate the optimal controller. The input of the controller is: 

.. math:: 

    e(t) = \bar{r}(t) - y(t)

The optimal controller must be such that when fed the virtual reference it produces the measured input (:math:`u(t)`). Thus: 

.. math::

    u(t) = C(z; \hat{\theta}) \bar{r}(t)

Where :math:`\hat{\theta}` is the optimal parameter vector. 

To calculate the optimal parameter vector we can start by attempting to solve a slightly easier problem. Apply a filter :math:`L(t)` to the data. We will explain the reason for this filter soon but bear with me in the meantime.

.. math::

    e_L(t) = L(z)e(t) \quad, \quad y_L(t) = L(z)e(t)

Now, consider the following performance index:  

.. math::

    \begin{aligned}
        J^N_{VR}(\theta) &= \frac{1}{N} \sum_{t=1}^{N} \left( u_L(t) - C(z; \theta) \ e_L(t) \right)^2 \\
                         &= \frac{1}{N} \sum_{t=1}^{N}\left( u_L(t) - \beta^T(z)\theta \ e_L(t) \right)^2 \\
                         &= \frac{1}{N} \sum_{t=1}^{N}\left( u_L(t) - \varphi_L^T(t)\theta \right)^2, \quad \varphi_L = \beta^T(z) \  e_L(t)
    \end{aligned}

Since this equation is quadratic in :math:`\theta` the optimal value :math:`\hat{\theta}_N` is an explicit function of the data: 

.. math:: 

    \hat{\theta}_N = \left[ \sum_t \phi(t) \phi(t)^T \right]^{-1} \sum_t \phi(t)u(t)

However, we still haven't solved the original problem of minimising :math:`J_{MR}`. This method though is useful because it is based purely on the data and is relatively easy to compute. 

Before continuing we need to make a digression. Let's rewrite :math:`J_{MR}` in the frequency domain

.. math:: 
    
    \begin{aligned}
        J_{MR}(\theta) &= \frac{1}{2 \pi} \cdot \int_{-\pi}^{\pi} \left| \frac{PC(\theta)}{1 + PC(\theta)} - M \right|^2 \left| W \right|^2 d\omega \\
                       &= \frac{1}{2 \pi} \cdot \int_{-\pi}^{\pi} \left| \frac{PC(\theta)}{1 + PC(\theta)} - \frac{PC_0}{1 + PC_0} \right|^2 \left| W \right|^2 d\omega \\
                       &= \frac{1}{2 \pi} \cdot \int_{-\pi}^{\pi} \left| \frac{PC(\theta)[1 + PC_0]     - PC_0[1 + PC(\theta)]}{[1 + PC(\theta)][1 + PC_0]} \right|^2 \left| W \right|^2 d\omega \\
                       &= \frac{1}{2 \pi} \cdot \int_{-\pi}^{\pi} \left| P \cdot \frac{C(\theta) - C_0}{[1 + PC(\theta)][1 + PC_0]} \right|^2 \left| W \right|^2 d\omega \\
    \end{aligned}

We can now go back to :math:`J_{VR}`. 

.. math:. 

    
    J^N_{VR}(\theta) &= \frac{1}{N} \sum_{t=1}^{N} \left( u_L(t) - C(z; \theta) \ e_L(t) \right)^2 
    

If :math:`u(t)` and :math:`y(t)` can be considered realisations of stationary stochastic processes then as the amount of data grows (:math:`N \rightarrow \infty`) the following holds: 

.. math:: 

    J^N_{VR}(\theta) \rightarrow J_{VR}(\theta)

And: 

.. math::
    
    \begin{aligned}
        J_{VR}(\theta) &= E \left[ \left( u_L(t) - C(z; \theta)e_L(t) \right)^2 \right] \\
                       &= E \left[ L(z) \cdot \left( u(t) - C(z; \theta)e(t) \right)^2 \right]
    \end{aligned}

We would like to get rid of :math:`e(t)` in the equation to make it depend only on our initial data: 

.. math::

    e(t) = \bar{r}(t) - y(t) = \bar{r}(t) - M_R(z)\bar{r}(t) = \left( 1 - M_R(z) \right) \cdot \bar{r}(t)

And since:

.. math:: 

    r(t) = \frac{y(t)}{M_R} = \frac{P}{M_R} \cdot u(t) 

We can rewrite the criterion as: 

.. math::

    \begin{aligned}
        J^N_{VR}(\theta) &= \frac{1}{N} \sum_{t=1}^{N} \left[u_L(t) - C(z; \theta) \cdot \left( 1 - M_R(z) \right) \cdot \frac{P}{M_R} \cdot u_L(t) \right]^2 \\
                         &= \frac{1}{N} \sum_{t=1}^{N} \left[ \left( L(z) \left( 1 - C(z; \theta) \cdot \frac{1 - M_R(z)}{M_R(z)} \cdot P \right) u_L(t)\right)^2 \right]
    \end{aligned} 

We also know that, by definition, :math:`M_R` is: 

.. math::

    M_R(z) = \frac{PC_0}{1 + PC_0} \quad \Rightarrow \quad 1 - M_R = \frac{1 + PC_0 - PC_0}{1 + PC_0} = \frac{1}{1 + PC_0}

Which brings us to: 

.. math::

    \begin{aligned}
        1 - C(\theta) \cdot \frac{1 - M_R}{M_R} \cdot P &= \frac{1}{M_R} \left( M_R - PC(\theta) \left[ 1 - M_R \right] \right) \\
            &= \frac{1}{M_R} \left( \frac{PC_0}{1 + PC_0} - \frac{PC(\theta)}{1 + PC_0} \right) \\
            &= \frac{1}{M_R} \left( P \cdot \frac{C_0 - C(\theta)}{1 + PC_0} \right) \\
            &= \frac{1}{M_R} P \cdot \left( C_0 - C(\theta) \right) \left( 1 - M_R \right)
    \end{aligned}

By subsituting this result back into :math:`J_{VR}` we obtain: 

.. math::

    J^N_{VR}(\theta) = E \left[ \left( \frac{L}{M_R} \cdot \left( C_0 - C(\theta) \right) \cdot \left( 1 - M_R \right) \right)^2 \right]

The frequency-domain representation of this criterion is:

.. math::

    J_{VR}(\theta) = \frac{1}{2\pi} \int_{-\pi}^{\pi} \left| P \cdot \left( C_0 - C(\theta) \right) \left( 1 - M_R \right) \cdot \frac{L}{M_R} \right|^2 \Phi_u \ d\omega

Where, :math:`\Phi_u` is the power density of :math:`u(t)`. 

If :math:`C_0(z) \in C(z; \theta)` and :math:`J_{VR}(\theta)` has a unique minimum then minimizing :math:`J_{VR}(\theta)` will always yield :math:`{C_0(z)}` independently of the value of :math:`L(z)`.

Notice the similarity between the expressions of :math:`J_{MR}` and :math:`J_{VR}`. With a proper choice of the pre-filter :math:`L(z)` we can make them equal. 

If:

.. math::

    \left| L \right|^2 = \left| \frac{M_R W}{1 + PC(\theta)} \right|^2 \cdot \frac{1}{\Phi_u}

Then, 

.. math:: 
    
    \begin{aligned}
        J_{VR}(\theta) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} \left| P \cdot \frac{C_0 - C(\theta)}{1 + PC_0} \cdot \frac{W}{1 + PC(\theta)} \right|^2 \ d\omega \\
                       &= \frac{1}{2 \pi} \int_{-\pi}^{\pi} \left| P \cdot \frac{C(\theta) - C_0}{[1 + PC(\theta)][1 + PC_0]} \right|^2 \left| W \right|^2 \ d\omega = J_{MR}(\theta)
    \end{aligned}

Unfortunately this pre-filter is not practical since it depends on knowing the plant model :math:`P`. However, if :math:`C(z; \theta)` is a good approximation of :math:`C_0(z)` then:

.. math::

    \left( 1 - M_R \right) = \frac{1}{1 + PC(z; \theta)} \approx \frac{1}{1 + PC_0(z)}

Thus, :math:`L(z)` can be written as: 

.. math::

    L(z) = \left| \left(1 - M_R \right) \cdot M_R \cdot W \right|^2 \cdot \frac{1}{\Phi_u}

This formulation of the pre-filter is completely known. This proves that :math:`J_{MR}` and :math:`J_{VR}` are equivalent and that by solving the simpler problem (:math:`J_{VR}`) we also solve the original problem (:math:`J_{MR}`).

Recap
-----

In conclusion, the VRFT method can be broken down into a set of relatively simple steps: 

* Choose a reference model :math:`M_R` and class of controllers :math:`C(z; \theta)`
* Calculate the pre-filter :math:`L(z)`
* Compute the optimal parameter vector :math:`\hat{\theta}_N`

The VRFT method can be extended to cases where the signal is noisy, we will discuss this in a later post. The VRFT method has also been extended to cascade control systems. This too will be the subject of a later post. 

